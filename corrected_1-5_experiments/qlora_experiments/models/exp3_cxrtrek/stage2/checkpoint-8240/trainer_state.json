{
  "best_global_step": 4941,
  "best_metric": 0.0826646015048027,
  "best_model_checkpoint": "/l/users/muhra.almahri/Surgical_COT/corrected_1-5_experiments/qlora_experiments/models/exp3_cxrtrek/stage2/checkpoint-4941",
  "epoch": 5.0,
  "eval_steps": 1647,
  "global_step": 8240,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0303524680350571,
      "grad_norm": 2.330239772796631,
      "learning_rate": 9.879032258064516e-06,
      "loss": 1.3192,
      "step": 50
    },
    {
      "epoch": 0.0607049360701142,
      "grad_norm": 1.162581205368042,
      "learning_rate": 1.995967741935484e-05,
      "loss": 0.8687,
      "step": 100
    },
    {
      "epoch": 0.0910574041051713,
      "grad_norm": 1.5858454704284668,
      "learning_rate": 3.0040322580645162e-05,
      "loss": 0.3161,
      "step": 150
    },
    {
      "epoch": 0.1214098721402284,
      "grad_norm": 0.9798578023910522,
      "learning_rate": 4.0120967741935485e-05,
      "loss": 0.1804,
      "step": 200
    },
    {
      "epoch": 0.1517623401752855,
      "grad_norm": 0.7469440698623657,
      "learning_rate": 4.999374374374375e-05,
      "loss": 0.1338,
      "step": 250
    },
    {
      "epoch": 0.1821148082103426,
      "grad_norm": 0.8384519815444946,
      "learning_rate": 4.968093093093093e-05,
      "loss": 0.1577,
      "step": 300
    },
    {
      "epoch": 0.2124672762453997,
      "grad_norm": 0.7363412380218506,
      "learning_rate": 4.936811811811812e-05,
      "loss": 0.1368,
      "step": 350
    },
    {
      "epoch": 0.2428197442804568,
      "grad_norm": 2.1996405124664307,
      "learning_rate": 4.905530530530531e-05,
      "loss": 0.1406,
      "step": 400
    },
    {
      "epoch": 0.2731722123155139,
      "grad_norm": 1.1226438283920288,
      "learning_rate": 4.8742492492492495e-05,
      "loss": 0.137,
      "step": 450
    },
    {
      "epoch": 0.303524680350571,
      "grad_norm": 0.7192972898483276,
      "learning_rate": 4.842967967967968e-05,
      "loss": 0.1371,
      "step": 500
    },
    {
      "epoch": 0.3338771483856281,
      "grad_norm": 2.0887534618377686,
      "learning_rate": 4.811686686686687e-05,
      "loss": 0.1122,
      "step": 550
    },
    {
      "epoch": 0.3642296164206852,
      "grad_norm": 0.9881108999252319,
      "learning_rate": 4.780405405405405e-05,
      "loss": 0.1196,
      "step": 600
    },
    {
      "epoch": 0.39458208445574233,
      "grad_norm": 0.5582419037818909,
      "learning_rate": 4.749124124124124e-05,
      "loss": 0.1076,
      "step": 650
    },
    {
      "epoch": 0.4249345524907994,
      "grad_norm": 0.8284144401550293,
      "learning_rate": 4.717842842842843e-05,
      "loss": 0.1235,
      "step": 700
    },
    {
      "epoch": 0.4552870205258565,
      "grad_norm": 0.996302604675293,
      "learning_rate": 4.686561561561561e-05,
      "loss": 0.1192,
      "step": 750
    },
    {
      "epoch": 0.4856394885609136,
      "grad_norm": 1.25566828250885,
      "learning_rate": 4.65528028028028e-05,
      "loss": 0.125,
      "step": 800
    },
    {
      "epoch": 0.5159919565959707,
      "grad_norm": 2.066035747528076,
      "learning_rate": 4.623998998998999e-05,
      "loss": 0.1186,
      "step": 850
    },
    {
      "epoch": 0.5463444246310278,
      "grad_norm": 0.9057461619377136,
      "learning_rate": 4.5927177177177175e-05,
      "loss": 0.1089,
      "step": 900
    },
    {
      "epoch": 0.5766968926660849,
      "grad_norm": 0.6577224135398865,
      "learning_rate": 4.5614364364364365e-05,
      "loss": 0.1029,
      "step": 950
    },
    {
      "epoch": 0.607049360701142,
      "grad_norm": 0.443399041891098,
      "learning_rate": 4.5301551551551554e-05,
      "loss": 0.0958,
      "step": 1000
    },
    {
      "epoch": 0.6374018287361991,
      "grad_norm": 0.9133844375610352,
      "learning_rate": 4.498873873873874e-05,
      "loss": 0.0882,
      "step": 1050
    },
    {
      "epoch": 0.6677542967712562,
      "grad_norm": 0.5178312659263611,
      "learning_rate": 4.467592592592593e-05,
      "loss": 0.1003,
      "step": 1100
    },
    {
      "epoch": 0.6981067648063133,
      "grad_norm": 0.8306249380111694,
      "learning_rate": 4.4363113113113116e-05,
      "loss": 0.0974,
      "step": 1150
    },
    {
      "epoch": 0.7284592328413704,
      "grad_norm": 1.5491116046905518,
      "learning_rate": 4.40503003003003e-05,
      "loss": 0.1082,
      "step": 1200
    },
    {
      "epoch": 0.7588117008764275,
      "grad_norm": 0.6326621174812317,
      "learning_rate": 4.373748748748749e-05,
      "loss": 0.087,
      "step": 1250
    },
    {
      "epoch": 0.7891641689114847,
      "grad_norm": 0.5847375392913818,
      "learning_rate": 4.342467467467468e-05,
      "loss": 0.1017,
      "step": 1300
    },
    {
      "epoch": 0.8195166369465418,
      "grad_norm": 1.106077790260315,
      "learning_rate": 4.311186186186186e-05,
      "loss": 0.1097,
      "step": 1350
    },
    {
      "epoch": 0.8498691049815988,
      "grad_norm": 1.7330172061920166,
      "learning_rate": 4.279904904904905e-05,
      "loss": 0.0935,
      "step": 1400
    },
    {
      "epoch": 0.8802215730166559,
      "grad_norm": 1.163062572479248,
      "learning_rate": 4.248623623623624e-05,
      "loss": 0.0886,
      "step": 1450
    },
    {
      "epoch": 0.910574041051713,
      "grad_norm": 1.124977946281433,
      "learning_rate": 4.217342342342342e-05,
      "loss": 0.0955,
      "step": 1500
    },
    {
      "epoch": 0.9409265090867701,
      "grad_norm": 0.8601083755493164,
      "learning_rate": 4.186061061061061e-05,
      "loss": 0.0886,
      "step": 1550
    },
    {
      "epoch": 0.9712789771218272,
      "grad_norm": 1.6614115238189697,
      "learning_rate": 4.15477977977978e-05,
      "loss": 0.1012,
      "step": 1600
    },
    {
      "epoch": 0.9998102970747809,
      "eval_loss": 0.09754957258701324,
      "eval_runtime": 1268.3393,
      "eval_samples_per_second": 4.485,
      "eval_steps_per_second": 4.485,
      "step": 1647
    },
    {
      "epoch": 1.0012140987214022,
      "grad_norm": 3.092329502105713,
      "learning_rate": 4.1234984984984985e-05,
      "loss": 0.0913,
      "step": 1650
    },
    {
      "epoch": 1.0315665667564593,
      "grad_norm": 0.29036128520965576,
      "learning_rate": 4.0922172172172175e-05,
      "loss": 0.0762,
      "step": 1700
    },
    {
      "epoch": 1.0619190347915164,
      "grad_norm": 0.47144123911857605,
      "learning_rate": 4.060935935935936e-05,
      "loss": 0.0823,
      "step": 1750
    },
    {
      "epoch": 1.0922715028265735,
      "grad_norm": 0.35454118251800537,
      "learning_rate": 4.029654654654655e-05,
      "loss": 0.0864,
      "step": 1800
    },
    {
      "epoch": 1.1226239708616306,
      "grad_norm": 2.408628225326538,
      "learning_rate": 3.998373373373374e-05,
      "loss": 0.1006,
      "step": 1850
    },
    {
      "epoch": 1.1529764388966879,
      "grad_norm": 1.2115081548690796,
      "learning_rate": 3.967092092092092e-05,
      "loss": 0.0931,
      "step": 1900
    },
    {
      "epoch": 1.183328906931745,
      "grad_norm": 2.0434694290161133,
      "learning_rate": 3.935810810810811e-05,
      "loss": 0.0901,
      "step": 1950
    },
    {
      "epoch": 1.213681374966802,
      "grad_norm": 0.9681733846664429,
      "learning_rate": 3.90452952952953e-05,
      "loss": 0.091,
      "step": 2000
    },
    {
      "epoch": 1.2440338430018592,
      "grad_norm": 0.7235303521156311,
      "learning_rate": 3.873248248248248e-05,
      "loss": 0.0875,
      "step": 2050
    },
    {
      "epoch": 1.2743863110369162,
      "grad_norm": 0.6419340372085571,
      "learning_rate": 3.841966966966967e-05,
      "loss": 0.0838,
      "step": 2100
    },
    {
      "epoch": 1.3047387790719733,
      "grad_norm": 0.3542417287826538,
      "learning_rate": 3.810685685685686e-05,
      "loss": 0.0675,
      "step": 2150
    },
    {
      "epoch": 1.3350912471070304,
      "grad_norm": 0.7731851935386658,
      "learning_rate": 3.7794044044044044e-05,
      "loss": 0.083,
      "step": 2200
    },
    {
      "epoch": 1.3654437151420875,
      "grad_norm": 1.1473169326782227,
      "learning_rate": 3.7481231231231234e-05,
      "loss": 0.0808,
      "step": 2250
    },
    {
      "epoch": 1.3957961831771446,
      "grad_norm": 0.7082926630973816,
      "learning_rate": 3.7168418418418424e-05,
      "loss": 0.0931,
      "step": 2300
    },
    {
      "epoch": 1.4261486512122017,
      "grad_norm": 0.9747217297554016,
      "learning_rate": 3.6855605605605606e-05,
      "loss": 0.0858,
      "step": 2350
    },
    {
      "epoch": 1.4565011192472588,
      "grad_norm": 0.5052419304847717,
      "learning_rate": 3.6542792792792796e-05,
      "loss": 0.0903,
      "step": 2400
    },
    {
      "epoch": 1.4868535872823159,
      "grad_norm": 1.095995545387268,
      "learning_rate": 3.6229979979979986e-05,
      "loss": 0.0819,
      "step": 2450
    },
    {
      "epoch": 1.517206055317373,
      "grad_norm": 0.4927012324333191,
      "learning_rate": 3.591716716716717e-05,
      "loss": 0.0923,
      "step": 2500
    },
    {
      "epoch": 1.54755852335243,
      "grad_norm": 0.7266902923583984,
      "learning_rate": 3.560435435435436e-05,
      "loss": 0.0815,
      "step": 2550
    },
    {
      "epoch": 1.5779109913874874,
      "grad_norm": 0.6183826923370361,
      "learning_rate": 3.529154154154155e-05,
      "loss": 0.0859,
      "step": 2600
    },
    {
      "epoch": 1.6082634594225445,
      "grad_norm": 0.9007717967033386,
      "learning_rate": 3.497872872872873e-05,
      "loss": 0.0772,
      "step": 2650
    },
    {
      "epoch": 1.6386159274576015,
      "grad_norm": 0.7489020228385925,
      "learning_rate": 3.466591591591592e-05,
      "loss": 0.0787,
      "step": 2700
    },
    {
      "epoch": 1.6689683954926586,
      "grad_norm": 0.9237314462661743,
      "learning_rate": 3.435310310310311e-05,
      "loss": 0.0803,
      "step": 2750
    },
    {
      "epoch": 1.6993208635277157,
      "grad_norm": 0.5270315408706665,
      "learning_rate": 3.404029029029029e-05,
      "loss": 0.0857,
      "step": 2800
    },
    {
      "epoch": 1.7296733315627728,
      "grad_norm": 0.8164371252059937,
      "learning_rate": 3.372747747747748e-05,
      "loss": 0.0757,
      "step": 2850
    },
    {
      "epoch": 1.76002579959783,
      "grad_norm": 0.6344901919364929,
      "learning_rate": 3.341466466466467e-05,
      "loss": 0.0872,
      "step": 2900
    },
    {
      "epoch": 1.790378267632887,
      "grad_norm": 1.3031282424926758,
      "learning_rate": 3.3101851851851855e-05,
      "loss": 0.0814,
      "step": 2950
    },
    {
      "epoch": 1.820730735667944,
      "grad_norm": 2.519998550415039,
      "learning_rate": 3.278903903903904e-05,
      "loss": 0.0873,
      "step": 3000
    },
    {
      "epoch": 1.8510832037030012,
      "grad_norm": 0.4046249985694885,
      "learning_rate": 3.247622622622623e-05,
      "loss": 0.0844,
      "step": 3050
    },
    {
      "epoch": 1.8814356717380583,
      "grad_norm": 0.2915637791156769,
      "learning_rate": 3.216341341341341e-05,
      "loss": 0.0878,
      "step": 3100
    },
    {
      "epoch": 1.9117881397731153,
      "grad_norm": 1.2094361782073975,
      "learning_rate": 3.18506006006006e-05,
      "loss": 0.0791,
      "step": 3150
    },
    {
      "epoch": 1.9421406078081724,
      "grad_norm": 0.5911664366722107,
      "learning_rate": 3.153778778778779e-05,
      "loss": 0.0779,
      "step": 3200
    },
    {
      "epoch": 1.9724930758432295,
      "grad_norm": 0.46730947494506836,
      "learning_rate": 3.122497497497497e-05,
      "loss": 0.0833,
      "step": 3250
    },
    {
      "epoch": 1.9992032477140798,
      "eval_loss": 0.08378836512565613,
      "eval_runtime": 1262.2944,
      "eval_samples_per_second": 4.507,
      "eval_steps_per_second": 4.507,
      "step": 3294
    },
    {
      "epoch": 2.0024281974428044,
      "grad_norm": 1.6268436908721924,
      "learning_rate": 3.091216216216216e-05,
      "loss": 0.083,
      "step": 3300
    },
    {
      "epoch": 2.0327806654778615,
      "grad_norm": 0.7531623244285583,
      "learning_rate": 3.059934934934935e-05,
      "loss": 0.0839,
      "step": 3350
    },
    {
      "epoch": 2.0631331335129186,
      "grad_norm": 0.9222885370254517,
      "learning_rate": 3.0286536536536538e-05,
      "loss": 0.0759,
      "step": 3400
    },
    {
      "epoch": 2.0934856015479757,
      "grad_norm": 0.563007652759552,
      "learning_rate": 2.9973723723723724e-05,
      "loss": 0.0744,
      "step": 3450
    },
    {
      "epoch": 2.123838069583033,
      "grad_norm": 0.44161882996559143,
      "learning_rate": 2.966091091091091e-05,
      "loss": 0.0741,
      "step": 3500
    },
    {
      "epoch": 2.15419053761809,
      "grad_norm": 0.9304388165473938,
      "learning_rate": 2.9348098098098097e-05,
      "loss": 0.0685,
      "step": 3550
    },
    {
      "epoch": 2.184543005653147,
      "grad_norm": 1.2308651208877563,
      "learning_rate": 2.9035285285285286e-05,
      "loss": 0.0656,
      "step": 3600
    },
    {
      "epoch": 2.2148954736882045,
      "grad_norm": 0.8354862928390503,
      "learning_rate": 2.8722472472472472e-05,
      "loss": 0.0853,
      "step": 3650
    },
    {
      "epoch": 2.245247941723261,
      "grad_norm": 2.104907751083374,
      "learning_rate": 2.840965965965966e-05,
      "loss": 0.0817,
      "step": 3700
    },
    {
      "epoch": 2.2756004097583187,
      "grad_norm": 1.4586563110351562,
      "learning_rate": 2.8096846846846848e-05,
      "loss": 0.0663,
      "step": 3750
    },
    {
      "epoch": 2.3059528777933758,
      "grad_norm": 0.5117185115814209,
      "learning_rate": 2.7784034034034035e-05,
      "loss": 0.0773,
      "step": 3800
    },
    {
      "epoch": 2.336305345828433,
      "grad_norm": 1.2426780462265015,
      "learning_rate": 2.747122122122122e-05,
      "loss": 0.0627,
      "step": 3850
    },
    {
      "epoch": 2.36665781386349,
      "grad_norm": 1.735329508781433,
      "learning_rate": 2.715840840840841e-05,
      "loss": 0.0804,
      "step": 3900
    },
    {
      "epoch": 2.397010281898547,
      "grad_norm": 0.6146185398101807,
      "learning_rate": 2.6845595595595597e-05,
      "loss": 0.0757,
      "step": 3950
    },
    {
      "epoch": 2.427362749933604,
      "grad_norm": 0.29063680768013,
      "learning_rate": 2.6532782782782783e-05,
      "loss": 0.0757,
      "step": 4000
    },
    {
      "epoch": 2.457715217968661,
      "grad_norm": 1.0003783702850342,
      "learning_rate": 2.6219969969969972e-05,
      "loss": 0.0695,
      "step": 4050
    },
    {
      "epoch": 2.4880676860037183,
      "grad_norm": 3.363694190979004,
      "learning_rate": 2.590715715715716e-05,
      "loss": 0.075,
      "step": 4100
    },
    {
      "epoch": 2.5184201540387754,
      "grad_norm": 0.7566872835159302,
      "learning_rate": 2.5594344344344345e-05,
      "loss": 0.0813,
      "step": 4150
    },
    {
      "epoch": 2.5487726220738325,
      "grad_norm": 0.6496095061302185,
      "learning_rate": 2.528153153153153e-05,
      "loss": 0.0731,
      "step": 4200
    },
    {
      "epoch": 2.5791250901088896,
      "grad_norm": 1.0746225118637085,
      "learning_rate": 2.496871871871872e-05,
      "loss": 0.0705,
      "step": 4250
    },
    {
      "epoch": 2.6094775581439467,
      "grad_norm": 4.637800693511963,
      "learning_rate": 2.4655905905905907e-05,
      "loss": 0.0806,
      "step": 4300
    },
    {
      "epoch": 2.6398300261790038,
      "grad_norm": 3.383896827697754,
      "learning_rate": 2.4343093093093093e-05,
      "loss": 0.0718,
      "step": 4350
    },
    {
      "epoch": 2.670182494214061,
      "grad_norm": 1.9186530113220215,
      "learning_rate": 2.4030280280280283e-05,
      "loss": 0.0692,
      "step": 4400
    },
    {
      "epoch": 2.700534962249118,
      "grad_norm": 0.42597270011901855,
      "learning_rate": 2.371746746746747e-05,
      "loss": 0.0612,
      "step": 4450
    },
    {
      "epoch": 2.730887430284175,
      "grad_norm": 0.8073520660400391,
      "learning_rate": 2.3404654654654655e-05,
      "loss": 0.0738,
      "step": 4500
    },
    {
      "epoch": 2.761239898319232,
      "grad_norm": 0.984865128993988,
      "learning_rate": 2.3091841841841845e-05,
      "loss": 0.0698,
      "step": 4550
    },
    {
      "epoch": 2.791592366354289,
      "grad_norm": 0.7866685390472412,
      "learning_rate": 2.277902902902903e-05,
      "loss": 0.0847,
      "step": 4600
    },
    {
      "epoch": 2.8219448343893463,
      "grad_norm": 1.7314155101776123,
      "learning_rate": 2.2466216216216218e-05,
      "loss": 0.0651,
      "step": 4650
    },
    {
      "epoch": 2.8522973024244034,
      "grad_norm": 0.6425576210021973,
      "learning_rate": 2.2153403403403404e-05,
      "loss": 0.087,
      "step": 4700
    },
    {
      "epoch": 2.8826497704594605,
      "grad_norm": 0.9594075679779053,
      "learning_rate": 2.1840590590590593e-05,
      "loss": 0.0711,
      "step": 4750
    },
    {
      "epoch": 2.9130022384945176,
      "grad_norm": 1.1105506420135498,
      "learning_rate": 2.152777777777778e-05,
      "loss": 0.0679,
      "step": 4800
    },
    {
      "epoch": 2.9433547065295746,
      "grad_norm": 0.32576432824134827,
      "learning_rate": 2.1214964964964966e-05,
      "loss": 0.0732,
      "step": 4850
    },
    {
      "epoch": 2.9737071745646317,
      "grad_norm": 0.6639583706855774,
      "learning_rate": 2.0902152152152152e-05,
      "loss": 0.0662,
      "step": 4900
    },
    {
      "epoch": 2.9985961983533787,
      "eval_loss": 0.0826646015048027,
      "eval_runtime": 1255.037,
      "eval_samples_per_second": 4.533,
      "eval_steps_per_second": 4.533,
      "step": 4941
    },
    {
      "epoch": 3.0036422961642066,
      "grad_norm": 1.2237049341201782,
      "learning_rate": 2.058933933933934e-05,
      "loss": 0.0891,
      "step": 4950
    },
    {
      "epoch": 3.0339947641992637,
      "grad_norm": 1.0749976634979248,
      "learning_rate": 2.0276526526526528e-05,
      "loss": 0.0658,
      "step": 5000
    },
    {
      "epoch": 3.0643472322343213,
      "grad_norm": 0.4926750957965851,
      "learning_rate": 1.9963713713713714e-05,
      "loss": 0.0683,
      "step": 5050
    },
    {
      "epoch": 3.0946997002693784,
      "grad_norm": 2.411540985107422,
      "learning_rate": 1.96509009009009e-05,
      "loss": 0.0654,
      "step": 5100
    },
    {
      "epoch": 3.1250521683044354,
      "grad_norm": 1.28274405002594,
      "learning_rate": 1.9338088088088087e-05,
      "loss": 0.0643,
      "step": 5150
    },
    {
      "epoch": 3.1554046363394925,
      "grad_norm": 0.18933680653572083,
      "learning_rate": 1.9025275275275276e-05,
      "loss": 0.0618,
      "step": 5200
    },
    {
      "epoch": 3.1857571043745496,
      "grad_norm": 0.8313879370689392,
      "learning_rate": 1.8712462462462463e-05,
      "loss": 0.0607,
      "step": 5250
    },
    {
      "epoch": 3.2161095724096067,
      "grad_norm": 1.606400966644287,
      "learning_rate": 1.839964964964965e-05,
      "loss": 0.0614,
      "step": 5300
    },
    {
      "epoch": 3.246462040444664,
      "grad_norm": 2.1230521202087402,
      "learning_rate": 1.808683683683684e-05,
      "loss": 0.0656,
      "step": 5350
    },
    {
      "epoch": 3.276814508479721,
      "grad_norm": 0.5071349740028381,
      "learning_rate": 1.7774024024024025e-05,
      "loss": 0.0744,
      "step": 5400
    },
    {
      "epoch": 3.307166976514778,
      "grad_norm": 5.308362007141113,
      "learning_rate": 1.746121121121121e-05,
      "loss": 0.064,
      "step": 5450
    },
    {
      "epoch": 3.337519444549835,
      "grad_norm": 0.4333619475364685,
      "learning_rate": 1.71483983983984e-05,
      "loss": 0.0701,
      "step": 5500
    },
    {
      "epoch": 3.367871912584892,
      "grad_norm": 1.1831165552139282,
      "learning_rate": 1.6835585585585587e-05,
      "loss": 0.0558,
      "step": 5550
    },
    {
      "epoch": 3.3982243806199492,
      "grad_norm": 1.222951889038086,
      "learning_rate": 1.6522772772772773e-05,
      "loss": 0.0707,
      "step": 5600
    },
    {
      "epoch": 3.4285768486550063,
      "grad_norm": 1.2771847248077393,
      "learning_rate": 1.6209959959959963e-05,
      "loss": 0.0694,
      "step": 5650
    },
    {
      "epoch": 3.4589293166900634,
      "grad_norm": 4.143576145172119,
      "learning_rate": 1.589714714714715e-05,
      "loss": 0.0683,
      "step": 5700
    },
    {
      "epoch": 3.4892817847251205,
      "grad_norm": 0.3524606227874756,
      "learning_rate": 1.5584334334334335e-05,
      "loss": 0.0587,
      "step": 5750
    },
    {
      "epoch": 3.5196342527601776,
      "grad_norm": 1.4063395261764526,
      "learning_rate": 1.527152152152152e-05,
      "loss": 0.0672,
      "step": 5800
    },
    {
      "epoch": 3.5499867207952347,
      "grad_norm": 0.8800516128540039,
      "learning_rate": 1.4958708708708711e-05,
      "loss": 0.0545,
      "step": 5850
    },
    {
      "epoch": 3.580339188830292,
      "grad_norm": 4.286424160003662,
      "learning_rate": 1.4645895895895897e-05,
      "loss": 0.0758,
      "step": 5900
    },
    {
      "epoch": 3.610691656865349,
      "grad_norm": 2.0325169563293457,
      "learning_rate": 1.4333083083083085e-05,
      "loss": 0.079,
      "step": 5950
    },
    {
      "epoch": 3.641044124900406,
      "grad_norm": 0.7376187443733215,
      "learning_rate": 1.4020270270270271e-05,
      "loss": 0.0624,
      "step": 6000
    },
    {
      "epoch": 3.671396592935463,
      "grad_norm": 2.2127585411071777,
      "learning_rate": 1.370745745745746e-05,
      "loss": 0.0675,
      "step": 6050
    },
    {
      "epoch": 3.70174906097052,
      "grad_norm": 0.7721596360206604,
      "learning_rate": 1.3394644644644647e-05,
      "loss": 0.0783,
      "step": 6100
    },
    {
      "epoch": 3.7321015290055772,
      "grad_norm": 1.696897029876709,
      "learning_rate": 1.3081831831831832e-05,
      "loss": 0.0657,
      "step": 6150
    },
    {
      "epoch": 3.7624539970406343,
      "grad_norm": 1.6991968154907227,
      "learning_rate": 1.2769019019019018e-05,
      "loss": 0.0554,
      "step": 6200
    },
    {
      "epoch": 3.7928064650756914,
      "grad_norm": 1.903012990951538,
      "learning_rate": 1.2456206206206208e-05,
      "loss": 0.0601,
      "step": 6250
    },
    {
      "epoch": 3.8231589331107485,
      "grad_norm": 3.063987970352173,
      "learning_rate": 1.2143393393393394e-05,
      "loss": 0.0702,
      "step": 6300
    },
    {
      "epoch": 3.8535114011458056,
      "grad_norm": 1.3679574728012085,
      "learning_rate": 1.1830580580580582e-05,
      "loss": 0.0633,
      "step": 6350
    },
    {
      "epoch": 3.8838638691808627,
      "grad_norm": 1.6548494100570679,
      "learning_rate": 1.1517767767767768e-05,
      "loss": 0.0698,
      "step": 6400
    },
    {
      "epoch": 3.9142163372159198,
      "grad_norm": 3.6730685234069824,
      "learning_rate": 1.1204954954954954e-05,
      "loss": 0.0717,
      "step": 6450
    },
    {
      "epoch": 3.944568805250977,
      "grad_norm": 1.0041221380233765,
      "learning_rate": 1.0892142142142142e-05,
      "loss": 0.0615,
      "step": 6500
    },
    {
      "epoch": 3.974921273286034,
      "grad_norm": 2.1065590381622314,
      "learning_rate": 1.0579329329329329e-05,
      "loss": 0.0672,
      "step": 6550
    },
    {
      "epoch": 3.9979891489926773,
      "eval_loss": 0.08538388460874557,
      "eval_runtime": 1259.6522,
      "eval_samples_per_second": 4.516,
      "eval_steps_per_second": 4.516,
      "step": 6588
    },
    {
      "epoch": 4.004856394885609,
      "grad_norm": 1.0960782766342163,
      "learning_rate": 1.0266516516516517e-05,
      "loss": 0.0515,
      "step": 6600
    },
    {
      "epoch": 4.035208862920666,
      "grad_norm": 0.8105334639549255,
      "learning_rate": 9.953703703703704e-06,
      "loss": 0.0604,
      "step": 6650
    },
    {
      "epoch": 4.065561330955723,
      "grad_norm": 0.9968749284744263,
      "learning_rate": 9.64089089089089e-06,
      "loss": 0.0644,
      "step": 6700
    },
    {
      "epoch": 4.095913798990781,
      "grad_norm": 1.1740864515304565,
      "learning_rate": 9.328078078078079e-06,
      "loss": 0.0597,
      "step": 6750
    },
    {
      "epoch": 4.126266267025837,
      "grad_norm": 2.3520984649658203,
      "learning_rate": 9.015265265265267e-06,
      "loss": 0.054,
      "step": 6800
    },
    {
      "epoch": 4.156618735060895,
      "grad_norm": 2.0433177947998047,
      "learning_rate": 8.702452452452453e-06,
      "loss": 0.0541,
      "step": 6850
    },
    {
      "epoch": 4.186971203095951,
      "grad_norm": 1.7241164445877075,
      "learning_rate": 8.38963963963964e-06,
      "loss": 0.0606,
      "step": 6900
    },
    {
      "epoch": 4.217323671131009,
      "grad_norm": 1.7697458267211914,
      "learning_rate": 8.076826826826827e-06,
      "loss": 0.0507,
      "step": 6950
    },
    {
      "epoch": 4.247676139166066,
      "grad_norm": 8.600747108459473,
      "learning_rate": 7.764014014014015e-06,
      "loss": 0.0501,
      "step": 7000
    },
    {
      "epoch": 4.278028607201123,
      "grad_norm": 0.7667341232299805,
      "learning_rate": 7.451201201201202e-06,
      "loss": 0.0649,
      "step": 7050
    },
    {
      "epoch": 4.30838107523618,
      "grad_norm": 0.7671477198600769,
      "learning_rate": 7.138388388388388e-06,
      "loss": 0.064,
      "step": 7100
    },
    {
      "epoch": 4.338733543271237,
      "grad_norm": 2.9025063514709473,
      "learning_rate": 6.825575575575575e-06,
      "loss": 0.0822,
      "step": 7150
    },
    {
      "epoch": 4.369086011306294,
      "grad_norm": 2.6475839614868164,
      "learning_rate": 6.5127627627627624e-06,
      "loss": 0.0541,
      "step": 7200
    },
    {
      "epoch": 4.3994384793413515,
      "grad_norm": 1.064576268196106,
      "learning_rate": 6.19994994994995e-06,
      "loss": 0.0647,
      "step": 7250
    },
    {
      "epoch": 4.429790947376409,
      "grad_norm": 0.7802569270133972,
      "learning_rate": 5.8871371371371375e-06,
      "loss": 0.0517,
      "step": 7300
    },
    {
      "epoch": 4.460143415411466,
      "grad_norm": 0.6483689546585083,
      "learning_rate": 5.5743243243243245e-06,
      "loss": 0.0516,
      "step": 7350
    },
    {
      "epoch": 4.490495883446522,
      "grad_norm": 1.3067922592163086,
      "learning_rate": 5.261511511511512e-06,
      "loss": 0.0618,
      "step": 7400
    },
    {
      "epoch": 4.52084835148158,
      "grad_norm": 3.1332147121429443,
      "learning_rate": 4.948698698698699e-06,
      "loss": 0.0541,
      "step": 7450
    },
    {
      "epoch": 4.551200819516637,
      "grad_norm": 6.529390811920166,
      "learning_rate": 4.635885885885887e-06,
      "loss": 0.0512,
      "step": 7500
    },
    {
      "epoch": 4.581553287551694,
      "grad_norm": 2.743692636489868,
      "learning_rate": 4.323073073073073e-06,
      "loss": 0.0752,
      "step": 7550
    },
    {
      "epoch": 4.6119057555867515,
      "grad_norm": 0.2801518440246582,
      "learning_rate": 4.01026026026026e-06,
      "loss": 0.0615,
      "step": 7600
    },
    {
      "epoch": 4.642258223621808,
      "grad_norm": 0.4583408534526825,
      "learning_rate": 3.6974474474474475e-06,
      "loss": 0.0632,
      "step": 7650
    },
    {
      "epoch": 4.672610691656866,
      "grad_norm": 0.962716281414032,
      "learning_rate": 3.384634634634635e-06,
      "loss": 0.0495,
      "step": 7700
    },
    {
      "epoch": 4.702963159691922,
      "grad_norm": 0.97136390209198,
      "learning_rate": 3.071821821821822e-06,
      "loss": 0.0622,
      "step": 7750
    },
    {
      "epoch": 4.73331562772698,
      "grad_norm": 1.7734920978546143,
      "learning_rate": 2.759009009009009e-06,
      "loss": 0.058,
      "step": 7800
    },
    {
      "epoch": 4.7636680957620365,
      "grad_norm": 0.7496230602264404,
      "learning_rate": 2.4461961961961963e-06,
      "loss": 0.0531,
      "step": 7850
    },
    {
      "epoch": 4.794020563797094,
      "grad_norm": 1.741010308265686,
      "learning_rate": 2.1333833833833838e-06,
      "loss": 0.0447,
      "step": 7900
    },
    {
      "epoch": 4.824373031832151,
      "grad_norm": 1.244642972946167,
      "learning_rate": 1.8205705705705707e-06,
      "loss": 0.0549,
      "step": 7950
    },
    {
      "epoch": 4.854725499867208,
      "grad_norm": 1.2173136472702026,
      "learning_rate": 1.5077577577577578e-06,
      "loss": 0.064,
      "step": 8000
    },
    {
      "epoch": 4.885077967902265,
      "grad_norm": 2.76710844039917,
      "learning_rate": 1.194944944944945e-06,
      "loss": 0.051,
      "step": 8050
    },
    {
      "epoch": 4.915430435937322,
      "grad_norm": 1.1892050504684448,
      "learning_rate": 8.821321321321322e-07,
      "loss": 0.0651,
      "step": 8100
    },
    {
      "epoch": 4.945782903972379,
      "grad_norm": 2.315122127532959,
      "learning_rate": 5.693193193193193e-07,
      "loss": 0.0579,
      "step": 8150
    },
    {
      "epoch": 4.976135372007437,
      "grad_norm": 0.5851513743400574,
      "learning_rate": 2.5650650650650653e-07,
      "loss": 0.0544,
      "step": 8200
    },
    {
      "epoch": 4.997382099631976,
      "eval_loss": 0.09115687757730484,
      "eval_runtime": 1255.5384,
      "eval_samples_per_second": 4.531,
      "eval_steps_per_second": 4.531,
      "step": 8235
    }
  ],
  "logging_steps": 50,
  "max_steps": 8240,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 1647,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.51418068238992e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
