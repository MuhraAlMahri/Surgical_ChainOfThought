{
  "best_global_step": 3294,
  "best_metric": 0.08129972219467163,
  "best_model_checkpoint": "/l/users/muhra.almahri/Surgical_COT/corrected_1-5_experiments/qlora_experiments/models/exp4_curriculum/stage2/checkpoint-3294",
  "epoch": 1.9992032477140798,
  "eval_steps": 1647,
  "global_step": 3294,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0303524680350571,
      "grad_norm": 6.956348896026611,
      "learning_rate": 9.879032258064516e-06,
      "loss": 0.1604,
      "step": 50
    },
    {
      "epoch": 0.0607049360701142,
      "grad_norm": 1.341762900352478,
      "learning_rate": 1.995967741935484e-05,
      "loss": 0.1142,
      "step": 100
    },
    {
      "epoch": 0.0910574041051713,
      "grad_norm": 7.130313873291016,
      "learning_rate": 3.0040322580645162e-05,
      "loss": 0.0869,
      "step": 150
    },
    {
      "epoch": 0.1214098721402284,
      "grad_norm": 1.5700466632843018,
      "learning_rate": 4.0120967741935485e-05,
      "loss": 0.098,
      "step": 200
    },
    {
      "epoch": 0.1517623401752855,
      "grad_norm": 1.1824007034301758,
      "learning_rate": 4.999374374374375e-05,
      "loss": 0.0823,
      "step": 250
    },
    {
      "epoch": 0.1821148082103426,
      "grad_norm": 0.9826145172119141,
      "learning_rate": 4.968093093093093e-05,
      "loss": 0.102,
      "step": 300
    },
    {
      "epoch": 0.2124672762453997,
      "grad_norm": 1.5308637619018555,
      "learning_rate": 4.936811811811812e-05,
      "loss": 0.0955,
      "step": 350
    },
    {
      "epoch": 0.2428197442804568,
      "grad_norm": 1.3020471334457397,
      "learning_rate": 4.905530530530531e-05,
      "loss": 0.0956,
      "step": 400
    },
    {
      "epoch": 0.2731722123155139,
      "grad_norm": 1.0275815725326538,
      "learning_rate": 4.8742492492492495e-05,
      "loss": 0.1062,
      "step": 450
    },
    {
      "epoch": 0.303524680350571,
      "grad_norm": 1.0090371370315552,
      "learning_rate": 4.842967967967968e-05,
      "loss": 0.11,
      "step": 500
    },
    {
      "epoch": 0.3338771483856281,
      "grad_norm": 1.369744062423706,
      "learning_rate": 4.811686686686687e-05,
      "loss": 0.0884,
      "step": 550
    },
    {
      "epoch": 0.3642296164206852,
      "grad_norm": 1.218400001525879,
      "learning_rate": 4.780405405405405e-05,
      "loss": 0.0949,
      "step": 600
    },
    {
      "epoch": 0.39458208445574233,
      "grad_norm": 1.58181893825531,
      "learning_rate": 4.749124124124124e-05,
      "loss": 0.089,
      "step": 650
    },
    {
      "epoch": 0.4249345524907994,
      "grad_norm": 0.8384928107261658,
      "learning_rate": 4.717842842842843e-05,
      "loss": 0.0989,
      "step": 700
    },
    {
      "epoch": 0.4552870205258565,
      "grad_norm": 1.473178744316101,
      "learning_rate": 4.686561561561561e-05,
      "loss": 0.099,
      "step": 750
    },
    {
      "epoch": 0.4856394885609136,
      "grad_norm": 2.095010757446289,
      "learning_rate": 4.65528028028028e-05,
      "loss": 0.1002,
      "step": 800
    },
    {
      "epoch": 0.5159919565959707,
      "grad_norm": 2.545806407928467,
      "learning_rate": 4.623998998998999e-05,
      "loss": 0.1044,
      "step": 850
    },
    {
      "epoch": 0.5463444246310278,
      "grad_norm": 1.9136894941329956,
      "learning_rate": 4.5927177177177175e-05,
      "loss": 0.0962,
      "step": 900
    },
    {
      "epoch": 0.5766968926660849,
      "grad_norm": 0.6613806486129761,
      "learning_rate": 4.5614364364364365e-05,
      "loss": 0.0888,
      "step": 950
    },
    {
      "epoch": 0.607049360701142,
      "grad_norm": 0.6916483044624329,
      "learning_rate": 4.5301551551551554e-05,
      "loss": 0.0837,
      "step": 1000
    },
    {
      "epoch": 0.6374018287361991,
      "grad_norm": 1.0297149419784546,
      "learning_rate": 4.498873873873874e-05,
      "loss": 0.0742,
      "step": 1050
    },
    {
      "epoch": 0.6677542967712562,
      "grad_norm": 1.0777782201766968,
      "learning_rate": 4.467592592592593e-05,
      "loss": 0.0921,
      "step": 1100
    },
    {
      "epoch": 0.6981067648063133,
      "grad_norm": 1.8677549362182617,
      "learning_rate": 4.4363113113113116e-05,
      "loss": 0.0796,
      "step": 1150
    },
    {
      "epoch": 0.7284592328413704,
      "grad_norm": 0.8536136746406555,
      "learning_rate": 4.40503003003003e-05,
      "loss": 0.0937,
      "step": 1200
    },
    {
      "epoch": 0.7588117008764275,
      "grad_norm": 0.7537126541137695,
      "learning_rate": 4.373748748748749e-05,
      "loss": 0.0761,
      "step": 1250
    },
    {
      "epoch": 0.7891641689114847,
      "grad_norm": 0.6938652396202087,
      "learning_rate": 4.342467467467468e-05,
      "loss": 0.0924,
      "step": 1300
    },
    {
      "epoch": 0.8195166369465418,
      "grad_norm": 1.2353296279907227,
      "learning_rate": 4.311186186186186e-05,
      "loss": 0.0942,
      "step": 1350
    },
    {
      "epoch": 0.8498691049815988,
      "grad_norm": 1.4172667264938354,
      "learning_rate": 4.279904904904905e-05,
      "loss": 0.0823,
      "step": 1400
    },
    {
      "epoch": 0.8802215730166559,
      "grad_norm": 2.239603042602539,
      "learning_rate": 4.248623623623624e-05,
      "loss": 0.0784,
      "step": 1450
    },
    {
      "epoch": 0.910574041051713,
      "grad_norm": 1.1124768257141113,
      "learning_rate": 4.217342342342342e-05,
      "loss": 0.0875,
      "step": 1500
    },
    {
      "epoch": 0.9409265090867701,
      "grad_norm": 0.7613360285758972,
      "learning_rate": 4.186061061061061e-05,
      "loss": 0.0829,
      "step": 1550
    },
    {
      "epoch": 0.9712789771218272,
      "grad_norm": 1.8762853145599365,
      "learning_rate": 4.15477977977978e-05,
      "loss": 0.0882,
      "step": 1600
    },
    {
      "epoch": 0.9998102970747809,
      "eval_loss": 0.08944525569677353,
      "eval_runtime": 1269.5866,
      "eval_samples_per_second": 4.481,
      "eval_steps_per_second": 4.481,
      "step": 1647
    },
    {
      "epoch": 1.0012140987214022,
      "grad_norm": 2.2787132263183594,
      "learning_rate": 4.1234984984984985e-05,
      "loss": 0.0855,
      "step": 1650
    },
    {
      "epoch": 1.0315665667564593,
      "grad_norm": 0.1963624358177185,
      "learning_rate": 4.0922172172172175e-05,
      "loss": 0.0698,
      "step": 1700
    },
    {
      "epoch": 1.0619190347915164,
      "grad_norm": 0.38921764492988586,
      "learning_rate": 4.060935935935936e-05,
      "loss": 0.071,
      "step": 1750
    },
    {
      "epoch": 1.0922715028265735,
      "grad_norm": 0.3522096574306488,
      "learning_rate": 4.029654654654655e-05,
      "loss": 0.0748,
      "step": 1800
    },
    {
      "epoch": 1.1226239708616306,
      "grad_norm": 1.9495328664779663,
      "learning_rate": 3.998373373373374e-05,
      "loss": 0.0902,
      "step": 1850
    },
    {
      "epoch": 1.1529764388966879,
      "grad_norm": 1.1147245168685913,
      "learning_rate": 3.967092092092092e-05,
      "loss": 0.0884,
      "step": 1900
    },
    {
      "epoch": 1.183328906931745,
      "grad_norm": 0.8926621675491333,
      "learning_rate": 3.935810810810811e-05,
      "loss": 0.0768,
      "step": 1950
    },
    {
      "epoch": 1.213681374966802,
      "grad_norm": 0.9469178915023804,
      "learning_rate": 3.90452952952953e-05,
      "loss": 0.0794,
      "step": 2000
    },
    {
      "epoch": 1.2440338430018592,
      "grad_norm": 0.5311209559440613,
      "learning_rate": 3.873248248248248e-05,
      "loss": 0.0814,
      "step": 2050
    },
    {
      "epoch": 1.2743863110369162,
      "grad_norm": 0.7549648284912109,
      "learning_rate": 3.841966966966967e-05,
      "loss": 0.0754,
      "step": 2100
    },
    {
      "epoch": 1.3047387790719733,
      "grad_norm": 1.2628313302993774,
      "learning_rate": 3.810685685685686e-05,
      "loss": 0.0633,
      "step": 2150
    },
    {
      "epoch": 1.3350912471070304,
      "grad_norm": 0.8939415216445923,
      "learning_rate": 3.7794044044044044e-05,
      "loss": 0.0703,
      "step": 2200
    },
    {
      "epoch": 1.3654437151420875,
      "grad_norm": 0.5705716013908386,
      "learning_rate": 3.7481231231231234e-05,
      "loss": 0.0743,
      "step": 2250
    },
    {
      "epoch": 1.3957961831771446,
      "grad_norm": 1.1134545803070068,
      "learning_rate": 3.7168418418418424e-05,
      "loss": 0.0842,
      "step": 2300
    },
    {
      "epoch": 1.4261486512122017,
      "grad_norm": 1.1674649715423584,
      "learning_rate": 3.6855605605605606e-05,
      "loss": 0.0807,
      "step": 2350
    },
    {
      "epoch": 1.4565011192472588,
      "grad_norm": 0.3973434269428253,
      "learning_rate": 3.6542792792792796e-05,
      "loss": 0.0827,
      "step": 2400
    },
    {
      "epoch": 1.4868535872823159,
      "grad_norm": 2.587263822555542,
      "learning_rate": 3.6229979979979986e-05,
      "loss": 0.0705,
      "step": 2450
    },
    {
      "epoch": 1.517206055317373,
      "grad_norm": 0.6264541745185852,
      "learning_rate": 3.591716716716717e-05,
      "loss": 0.0843,
      "step": 2500
    },
    {
      "epoch": 1.54755852335243,
      "grad_norm": 1.0003265142440796,
      "learning_rate": 3.560435435435436e-05,
      "loss": 0.0765,
      "step": 2550
    },
    {
      "epoch": 1.5779109913874874,
      "grad_norm": 1.3704372644424438,
      "learning_rate": 3.529154154154155e-05,
      "loss": 0.0763,
      "step": 2600
    },
    {
      "epoch": 1.6082634594225445,
      "grad_norm": 1.022474765777588,
      "learning_rate": 3.497872872872873e-05,
      "loss": 0.0725,
      "step": 2650
    },
    {
      "epoch": 1.6386159274576015,
      "grad_norm": 0.43189823627471924,
      "learning_rate": 3.466591591591592e-05,
      "loss": 0.0735,
      "step": 2700
    },
    {
      "epoch": 1.6689683954926586,
      "grad_norm": 2.156155586242676,
      "learning_rate": 3.435310310310311e-05,
      "loss": 0.0724,
      "step": 2750
    },
    {
      "epoch": 1.6993208635277157,
      "grad_norm": 0.648673415184021,
      "learning_rate": 3.404029029029029e-05,
      "loss": 0.0783,
      "step": 2800
    },
    {
      "epoch": 1.7296733315627728,
      "grad_norm": 1.0519061088562012,
      "learning_rate": 3.372747747747748e-05,
      "loss": 0.0686,
      "step": 2850
    },
    {
      "epoch": 1.76002579959783,
      "grad_norm": 0.8164758086204529,
      "learning_rate": 3.341466466466467e-05,
      "loss": 0.0849,
      "step": 2900
    },
    {
      "epoch": 1.790378267632887,
      "grad_norm": 1.0456104278564453,
      "learning_rate": 3.3101851851851855e-05,
      "loss": 0.0743,
      "step": 2950
    },
    {
      "epoch": 1.820730735667944,
      "grad_norm": 4.959470748901367,
      "learning_rate": 3.278903903903904e-05,
      "loss": 0.0792,
      "step": 3000
    },
    {
      "epoch": 1.8510832037030012,
      "grad_norm": 0.3784520924091339,
      "learning_rate": 3.247622622622623e-05,
      "loss": 0.0763,
      "step": 3050
    },
    {
      "epoch": 1.8814356717380583,
      "grad_norm": 0.2923697531223297,
      "learning_rate": 3.216341341341341e-05,
      "loss": 0.0798,
      "step": 3100
    },
    {
      "epoch": 1.9117881397731153,
      "grad_norm": 0.46487417817115784,
      "learning_rate": 3.18506006006006e-05,
      "loss": 0.0749,
      "step": 3150
    },
    {
      "epoch": 1.9421406078081724,
      "grad_norm": 0.8705036640167236,
      "learning_rate": 3.153778778778779e-05,
      "loss": 0.0727,
      "step": 3200
    },
    {
      "epoch": 1.9724930758432295,
      "grad_norm": 0.4697013795375824,
      "learning_rate": 3.122497497497497e-05,
      "loss": 0.0768,
      "step": 3250
    },
    {
      "epoch": 1.9992032477140798,
      "eval_loss": 0.08129972219467163,
      "eval_runtime": 1270.2151,
      "eval_samples_per_second": 4.479,
      "eval_steps_per_second": 4.479,
      "step": 3294
    }
  ],
  "logging_steps": 50,
  "max_steps": 8240,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 1647,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.2043458315090227e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
