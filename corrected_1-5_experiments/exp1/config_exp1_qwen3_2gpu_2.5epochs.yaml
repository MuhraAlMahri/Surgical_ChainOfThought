# Experiment 1: Random Baseline with Qwen3-VL-8B - 2 GPUs, 2.5 Epochs
# Fast training setup with good quality

model_name: Qwen/Qwen3-VL-8B-Instruct

vision_frozen: true

lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]

train:
  seed: 42
  max_seq_len: 2900  # Full resolution: Image needs 2622 + text ~250 = 2870, use 2900 for safety
  train_bs: 1  # Per-device batch size (2 GPUs × 1 = 2 total)
  grad_accum: 8  # Effective batch = 2×1×8 = 16 (same as 4-GPU setup)
  eval_bs: 1
  lr: 5.0e-6
  weight_decay: 0.0
  epochs: 2.5  # Good balance of quality and speed
  warmup_ratio: 0.0
  bf16: true
  gradient_checkpointing: true
  logging_steps: 10
  save_steps: 2000
  eval_steps: 2000

data:
  train_jsonl: datasets/kvasir_ULTRA_CONDENSED/train_CATEGORY_BASED.jsonl
  val_jsonl: datasets/kvasir_ULTRA_CONDENSED/val_CATEGORY_BASED.jsonl
  image_root: /l/users/muhra.almahri/Surgical_COT/datasets/Kvasir-VQA/raw/images
  use_vision_cache: false

eval:
  numeric_rel_tol: 0.05
  numeric_abs_tol: 0.0

# ============================================================================
# EXP1: RANDOM BASELINE (2-GPU, 2.5 EPOCHS)
# ============================================================================
# - 2 GPUs: ~1.8x speedup (80-90% efficiency)
# - Full resolution: 2,900 tokens (no quality loss)
# - 2.5 epochs: Excellent quality, faster than 3 epochs
# - Expected time: ~22-23 hours
# - Memory per GPU: ~10GB (fits easily in 40GB)
# ============================================================================




