# Experiment 1: Instruction-Tuned Model with ULTRA-CONDENSED Dataset
# Training configuration for advisor-approved ultra-condensed instruction templates
# UPDATED: Using Qwen2-VL-7B-Instruct (upgraded from Qwen2-VL-7B)

model_name: Qwen/Qwen2-VL-7B-Instruct

vision_frozen: true

lora:
  r: 4  # Reduced from 8 to fit 8B model with full resolution in 40GB GPU
  alpha: 8  # Keep 2x ratio
  dropout: 0.05
  target_modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]

train:
  seed: 42
  max_seq_len: 2900  # Full resolution: Image needs 2622 + text ~250 = 2870
  train_bs: 1  # Must use 1 due to long sequences
  grad_accum: 16  # Effective batch size = 16
  eval_bs: 1
  lr: 5.0e-6
  weight_decay: 0.0
  epochs: 3
  warmup_ratio: 0.0
  bf16: true
  gradient_checkpointing: true  # Critical for memory savings
  logging_steps: 10
  save_steps: 2000  # Reduced eval frequency to save time
  eval_steps: 2000  # Eval only 4 times instead of 15 times

data:
  train_jsonl: datasets/kvasir_ULTRA_CONDENSED/train_CATEGORY_BASED.jsonl
  val_jsonl: datasets/kvasir_ULTRA_CONDENSED/val_CATEGORY_BASED.jsonl
  image_root: /l/users/muhra.almahri/Surgical_COT/datasets/Kvasir-VQA/raw/images
  use_vision_cache: false  # Disabled - caching has bugs, use direct processing
  # vision_cache_dir: exp1/vision_cache_v2
  # resolution_id: 1036

eval:
  numeric_rel_tol: 0.05
  numeric_abs_tol: 0.0

# ============================================================================
# NOTES
# ============================================================================
# - Using the CATEGORY_BASED dataset with advisor-approved instruction templates
# - Each instruction includes explicit candidate lists for close-ended questions
# - Multi-label vs. single-choice classification is properly distinguished
# - Increased max_seq_len to 1024 to accommodate longer instruction templates
# - Training parameters match the original Exp1 successful configuration
# ============================================================================

