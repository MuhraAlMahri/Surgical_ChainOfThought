# Qwen3-VL with 4 GPUs - 12 Hour Fast Training
# Full resolution, 2.5 epochs to hit 12-hour target

model_name: Qwen/Qwen3-VL-8B-Instruct

vision_frozen: true

lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]

train:
  seed: 42
  max_seq_len: 2900  # Full resolution
  train_bs: 1  # Per-device (4 GPUs × 1 = 4 total)
  grad_accum: 4  # Effective batch = 4×1×4 = 16
  eval_bs: 1
  lr: 5.0e-6
  weight_decay: 0.0
  epochs: 3  # Full training for best quality
  warmup_ratio: 0.0
  bf16: true
  gradient_checkpointing: true
  logging_steps: 10
  save_steps: 2000
  eval_steps: 2000

data:
  train_jsonl: datasets/kvasir_ULTRA_CONDENSED/train_CATEGORY_BASED.jsonl
  val_jsonl: datasets/kvasir_ULTRA_CONDENSED/val_CATEGORY_BASED.jsonl
  image_root: /l/users/muhra.almahri/Surgical_COT/datasets/Kvasir-VQA/raw/images
  use_vision_cache: false

eval:
  numeric_rel_tol: 0.05
  numeric_abs_tol: 0.0

# ============================================================================
# 4-GPU FAST TRAINING
# ============================================================================
# - 4 GPUs: ~3.6x speedup (80-90% efficiency)
# - Full resolution: 2,900 tokens (no quality loss)
# - 3 epochs: Full training for maximum quality
# - Expected time: ~14 hours
# - Memory per GPU: ~10GB (fits easily in 40GB)
# ============================================================================

