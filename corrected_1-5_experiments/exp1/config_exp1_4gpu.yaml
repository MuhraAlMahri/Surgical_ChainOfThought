# Experiment 1: Multi-GPU Training (4 GPUs)
# ULTRA-CONDENSED instructions with distributed training

model_name: Qwen/Qwen2-VL-7B-Instruct  # Updated from Qwen2-VL-7B

vision_frozen: true

lora:
  r: 16  # Can increase from 8 since we have more memory across 4 GPUs
  alpha: 32
  dropout: 0.05
  target_modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]

train:
  seed: 42
  max_seq_len: 2800
  train_bs: 1  # Per-device batch size (4 GPUs × 1 = 4 total)
  grad_accum: 4  # Reduced from 16 since we have 4 GPUs (effective batch = 4×1×4 = 16)
  eval_bs: 1
  lr: 5.0e-6
  weight_decay: 0.0
  epochs: 3
  warmup_ratio: 0.0
  bf16: true
  gradient_checkpointing: true
  logging_steps: 10
  save_steps: 500

data:
  train_jsonl: datasets/kvasir_ULTRA_CONDENSED/train_CATEGORY_BASED.jsonl
  val_jsonl: datasets/kvasir_ULTRA_CONDENSED/val_CATEGORY_BASED.jsonl
  image_root: /l/users/muhra.almahri/Surgical_COT/datasets/Kvasir-VQA/raw/images

eval:
  numeric_rel_tol: 0.05
  numeric_abs_tol: 0.0

# ============================================================================
# MULTI-GPU NOTES
# ============================================================================
# - Using 4 GPUs with DistributedDataParallel (DDP)
# - Effective batch size: 4 GPUs × 1 per-device × 4 grad_accum = 16 (same as single GPU)
# - Expected speedup: ~3.5-4x faster than single GPU
# - Estimated time: 46h ÷ 3.5 = ~13 hours (close to your usual 10h)
# ============================================================================

