# Experiment 1: Random Baseline Configuration
# ACTUAL parameters used in the original training

model_name: Qwen/Qwen2-VL-7B-Instruct  # Updated from Qwen2-VL-7B

vision_frozen: true

lora:
  r: 32                      # Actual LoRA rank used
  alpha: 64                  # Actual LoRA alpha used
  dropout: 0.05
  target_modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]

train:
  seed: 42
  max_seq_len: 512
  train_bs: 1                # Actual batch size used
  grad_accum: 16             # Effective batch size = 1 * 16 = 16
  eval_bs: 1
  lr: 5.0e-6                 # Actual learning rate (5e-6, not 1e-4)
  weight_decay: 0.01
  epochs: 3                  # Actual: 3 epochs, not 1
  warmup_ratio: 0.0          # No warmup used
  bf16: true
  gradient_checkpointing: false  # Disabled for VL models
  logging_steps: 10
  save_steps: 500
  eval_steps: 500

data:
  # Actual paths used in training
  train_file: datasets/kvasir_raw_6500_image_level_70_15_15/train.json
  val_file: datasets/kvasir_raw_6500_image_level_70_15_15/val.json
  test_file: datasets/kvasir_raw_6500_image_level_70_15_15/test.json
  image_root: /l/users/muhra.almahri/Surgical_COT/datasets/Kvasir-VQA/raw/images
  dataset_type: reordered
  image_max_size: 448        # Images resized to 448x448

output:
  model_dir: models/exp1_random_baseline
  results_dir: results

eval:
  numeric_rel_tol: 0.05
  numeric_abs_tol: 0.0

# ============================================================================
# TRAINING DETAILS
# ============================================================================
# Hardware: NVIDIA A100 GPU (single GPU)
# Memory: 128GB RAM allocated
# Training Time: ~3 hours
# Total Steps: ~3 epochs over 8,984 training samples
# Optimizer: AdamW
# Scheduler: Linear decay
# Precision: BFloat16
#
# Results:
#   - Final Accuracy: 20.31% (concise)
#   - This is the baseline for all experiments
# ============================================================================

