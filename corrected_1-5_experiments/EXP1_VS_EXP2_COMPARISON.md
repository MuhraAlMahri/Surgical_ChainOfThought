# Experiment 1 vs Experiment 2 Comparison

## Quick Reference

| Feature | Exp1: Random Baseline | Exp2: Curriculum Learning |
|---------|----------------------|---------------------------|
| **Model** | Qwen2-VL-7B (7B params) | **Qwen3-VL-8B** (8B params) |
| **Strategy** | Random order | **3-stage curriculum** (easyâ†’hard) |
| **Training Stages** | 1 stage | **3 stages** (progressive) |
| **Resolution** | 768Ã—768 letterbox | 768Ã—768 letterbox |
| **GPUs** | 2 GPUs | 2 GPUs |
| **Training Time** | ~14 hours | **~42 hours** (14h Ã— 3 stages) |
| **Expected Accuracy** | ~22-23% | **~24-27%** (+3-5%) |
| **Instructions** | Standard | ULTRA_CONDENSED |
| **Data Splits** | Image-level (no leakage) | Image-level (no leakage) |

---

## ğŸ¯ When to Use Each

### Use Exp1 (Random Baseline) When:
- âœ… Need quick results (~14 hours)
- âœ… Want simple baseline comparison
- âœ… Testing infrastructure
- âœ… Limited time/resources

### Use Exp2 (Curriculum Learning) When:
- âœ… Want best possible accuracy
- âœ… Can afford 42 hours training
- âœ… Need production model
- âœ… Research on curriculum learning

---

## ğŸ“Š Training Comparison

### Exp1: Single Stage
```
Base Model (Qwen2-VL-7B)
    â†“
Random Training (all questions mixed)
    â†“
Final Model (~14 hours)
```

### Exp2: Three Stages
```
Base Model (Qwen3-VL-8B)
    â†“
Stage 1: Initial Assessment (~14h)
    â†“
Stage 2: Findings Identification (~14h)
    â†“
Stage 3: Clinical Context (~14h)
    â†“
Final Model (~42 hours total)
```

---

## â±ï¸ Time Investment

### Exp1: Random Baseline
| Phase | Time |
|-------|------|
| Training | 14 hours |
| **Total** | **14 hours** |

### Exp2: Curriculum Learning
| Stage | Time | Cumulative |
|-------|------|------------|
| Stage 1 | 14 hours | 14 hours |
| Stage 2 | 14 hours | 28 hours |
| Stage 3 | 14 hours | **42 hours** |

**Additional time**: +28 hours (3Ã— longer)  
**Expected improvement**: +3-5% accuracy

---

## ğŸ’° Cost-Benefit Analysis

### Exp1
- **GPU-hours**: 2 GPUs Ã— 14h = **28 GPU-hours**
- **Wall-clock time**: 14 hours
- **Accuracy**: ~22-23%
- **Cost per accuracy point**: ~1.2 GPU-hours per %

### Exp2
- **GPU-hours**: 2 GPUs Ã— 42h = **84 GPU-hours**
- **Wall-clock time**: 42 hours (sequential)
- **Accuracy**: ~24-27%
- **Cost per accuracy point**: ~3.4 GPU-hours per %

**Verdict**: Exp1 is more cost-efficient per accuracy point, but Exp2 achieves higher absolute accuracy.

---

## ğŸš€ How to Run Both

### Run Exp1 First (Quick Baseline)
```bash
cd /l/users/muhra.almahri/Surgical_COT/corrected_1-5_experiments/exp1
sbatch slurm/train_exp1_768_letterbox_2gpu.slurm
```

**Complete in**: ~14 hours  
**Use for**: Baseline comparison

---

### Then Run Exp2 (Best Performance)
```bash
cd /l/users/muhra.almahri/Surgical_COT/corrected_1-5_experiments/exp2
./submit_all_stages.sh
```

**Complete in**: ~42 hours  
**Use for**: Production model

---

### Or Run Both in Parallel

If you have **4 GPUs** available:
```bash
# Terminal 1: Start Exp1 on GPUs 0-1
cd exp1
sbatch slurm/train_exp1_768_letterbox_2gpu.slurm

# Terminal 2: Start Exp2 Stage 1 on GPUs 2-3
cd exp2
# Modify SLURM scripts to use --gres=gpu:2 --constraint=gpu_2_3
sbatch slurm/train_stage1_2gpu.slurm
```

**Complete both in**: ~42 hours (Exp2 time, since it's longer)

---

## ğŸ“ˆ Expected Results Summary

### Baseline (Exp1)
- **Stage 1 Accuracy**: ~30%
- **Stage 2 Accuracy**: ~18%
- **Stage 3 Accuracy**: ~8%
- **Overall Accuracy**: ~22-23%

### Curriculum (Exp2)
- **Stage 1 Accuracy**: ~33%
- **Stage 2 Accuracy**: ~20%
- **Stage 3 Accuracy**: ~10%
- **Overall Accuracy**: ~24-27%

**Improvement**: Curriculum learning helps especially on harder stages (2 & 3)

---

## ğŸ“ Research Questions

### What Exp1 Answers
- â“ How well does Qwen2-VL perform on random endoscopic VQA?
- â“ Does 768Ã—768 letterbox work better than 448Ã—448?
- â“ Baseline performance for comparison

### What Exp2 Answers
- â“ Does curriculum learning improve over random training?
- â“ How much benefit from progressive difficulty?
- â“ Does Qwen3-VL-8B perform better than Qwen2-VL-7B?
- â“ What's the performance ceiling with best practices?

---

## ğŸ“‚ Directory Structure

```
corrected_1-5_experiments/
â”œâ”€â”€ exp1/                          # Random Baseline
â”‚   â”œâ”€â”€ train_exp1.py
â”‚   â”œâ”€â”€ dataset.py
â”‚   â”œâ”€â”€ templates.py
â”‚   â”œâ”€â”€ config_exp1_768_letterbox_2gpu.yaml
â”‚   â”œâ”€â”€ slurm/
â”‚   â”‚   â””â”€â”€ train_exp1_768_letterbox_2gpu.slurm
â”‚   â””â”€â”€ outputs/                   # Final checkpoint here
â”‚
â””â”€â”€ exp2/                          # Curriculum Learning
    â”œâ”€â”€ train_exp2_curriculum.py
    â”œâ”€â”€ dataset.py
    â”œâ”€â”€ templates.py
    â”œâ”€â”€ config_exp2_curriculum_2gpu.yaml
    â”œâ”€â”€ slurm/
    â”‚   â”œâ”€â”€ train_stage1_2gpu.slurm
    â”‚   â”œâ”€â”€ train_stage2_2gpu.slurm
    â”‚   â””â”€â”€ train_stage3_2gpu.slurm
    â”œâ”€â”€ submit_all_stages.sh
    â””â”€â”€ outputs/
        â”œâ”€â”€ stage1/                # Stage 1 checkpoint
        â”œâ”€â”€ stage2/                # Stage 2 checkpoint
        â””â”€â”€ stage3/                # Final checkpoint
```

---

## âœ¨ Recommendation

### For Quick Testing
**Run Exp1 first** to:
- Verify infrastructure works
- Get baseline results quickly
- Test evaluation pipeline

### For Best Results
**Then run Exp2** to:
- Achieve best accuracy
- Publish/present results
- Deploy in production

### Timeline
```
Day 0: Submit Exp1
Day 1 (14h later): Exp1 completes â†’ Submit Exp2
Day 3 (42h later): Exp2 completes
Total: ~56 hours (~2.3 days) for both experiments
```

---

*Comparison Date: November 11, 2025*  
*Both experiments use 768Ã—768 letterbox, 2 GPUs, image-level splits*






