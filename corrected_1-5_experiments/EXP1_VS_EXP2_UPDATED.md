# Experiment 1 vs Experiment 2 Comparison (UPDATED)

## Quick Reference

| Feature | Exp1: Random Baseline | Exp2: Qwen Reordered |
|---------|----------------------|----------------------|
| **Model** | Qwen2-VL-7B (7B params) | **Qwen3-VL-8B (8B params)** |
| **Data Order** | Random | **Qwen clinical stages (1â†’2â†’3)** |
| **Instructions** | Standard | **ULTRA_CONDENSED (363 chars)** |
| **Training** | 1 session | 1 session |
| **Training Time** | ~14 hours | ~14 hours |
| **Resolution** | 768Ã—768 letterbox | 768Ã—768 letterbox |
| **GPUs** | 2 GPUs | 2 GPUs |
| **Expected Accuracy** | ~22-23% | **~24-26%** (+2-4%) |
| **Data Splits** | Image-level (no leakage) | Image-level (no leakage) |
| **Complexity** | Simple baseline | Intelligently ordered |

---

## ğŸ¯ Key Difference

### Exp1: Random Order
Questions presented in **random shuffle**:
```
Q54: What abnormalities? â†’ Q12: Is there text? â†’ Q3: What procedure? â†’ ...
```
No logical flow, just random order.

### Exp2: Qwen Reordered
Questions presented in **clinical stages** (as determined by Qwen):
```
Stage 1 (35%): Quality/Procedure questions first
Stage 2 (64%): Findings/Instruments questions middle  
Stage 3 (0.1%): Clinical/Diagnosis questions last
```
Logical clinical workflow ordering.

---

## ğŸ“Š Data Comparison

### Exp1 (Random)
- **Train**: 41,079 QA pairs (random order)
- **Val**: 8,786 QA pairs (random order)
- **Test**: 8,984 QA pairs (random order)

### Exp2 (Qwen Reordered)
- **Train**: 41,079 QA pairs (Qwen stage order)
  - Stage 1: 14,679 (35.7%)
  - Stage 2: 26,357 (64.2%)
  - Stage 3: 43 (0.1%)
- **Val**: 8,786 QA pairs (Qwen stage order)
- **Test**: 8,984 QA pairs (Qwen stage order)

**Same data, different ordering!**

---

## â±ï¸ Time Investment

### Both Experiments
- **Training time**: ~14 hours each on 2 GPUs
- **Wall-clock time**: 14 hours
- **GPU-hours**: 28 GPU-hours each (2 GPUs Ã— 14h)

**Time efficient**: No additional time cost for exp2 vs exp1!

---

## ğŸ’¡ Research Hypothesis

**Question**: Does intelligent ordering (by Qwen) improve learning vs random order?

**If Exp2 > Exp1 significantly**:
- âœ… Ordering matters
- âœ… Clinical workflow helps model learn
- âœ… Qwen's stage classification is meaningful

**If Exp2 â‰ˆ Exp1**:
- Model learns regardless of order
- Or: Both Qwen3-8B and ordering cancel out differences

---

## ğŸš€ How to Run Both

### Option 1: Run Sequentially (Recommended)
```bash
# Start Exp1 first (baseline)
cd exp1
sbatch slurm/train_exp1_768_letterbox_2gpu.slurm

# Wait ~14 hours, then start Exp2
cd ../exp2
sbatch slurm/train_exp2_qwen_reordered_2gpu.slurm

# Total time: ~28 hours
```

### Option 2: Run in Parallel (If you have 4 GPUs)
```bash
# Terminal 1: Exp1 on GPUs 0-1
cd exp1
sbatch slurm/train_exp1_768_letterbox_2gpu.slurm

# Terminal 2: Exp2 on GPUs 2-3
cd exp2
# (Modify SLURM script to use different GPUs)
sbatch slurm/train_exp2_qwen_reordered_2gpu.slurm

# Both complete in ~14 hours
```

---

## ğŸ“ˆ Expected Results

### Baseline (Exp1)
- **Overall Accuracy**: ~22-23%
- **Model**: Qwen2-VL-7B
- **Order**: Random

### Qwen Reordered (Exp2)
- **Overall Accuracy**: ~24-26%
- **Model**: Qwen3-VL-8B (larger + newer)
- **Order**: Qwen clinical stages

**Expected improvement sources**:
1. **Better ordering** (+1-2%)
2. **Larger model** (+1-2%)
3. **ULTRA_CONDENSED instructions** (+0.5-1%)

**Total**: +2-4% improvement

---

## ğŸ”¬ What Each Experiment Tests

### Exp1 Tests
- â“ Baseline performance with random ordering
- â“ Qwen2-VL-7B capability on endoscopic VQA
- â“ 768Ã—768 letterbox effectiveness

### Exp2 Tests
- â“ Does Qwen's clinical ordering help?
- â“ Qwen3-VL-8B vs Qwen2-VL-7B performance
- â“ ULTRA_CONDENSED instructions effectiveness
- â“ Stage-based presentation benefits

---

## ğŸ“‚ Directory Structure

```
corrected_1-5_experiments/
â”œâ”€â”€ exp1/                                    # Random Baseline
â”‚   â”œâ”€â”€ train_exp1.py
â”‚   â”œâ”€â”€ config_exp1_768_letterbox_2gpu.yaml
â”‚   â”œâ”€â”€ slurm/train_exp1_768_letterbox_2gpu.slurm
â”‚   â””â”€â”€ outputs/                             # Checkpoint
â”‚
â”œâ”€â”€ exp2/                                    # Qwen Reordered
â”‚   â”œâ”€â”€ prepare_qwen_reordered_data.py       # Data prep script
â”‚   â”œâ”€â”€ train_exp2_qwen_reordered.py
â”‚   â”œâ”€â”€ config_exp2_qwen_reordered_2gpu.yaml
â”‚   â”œâ”€â”€ slurm/train_exp2_qwen_reordered_2gpu.slurm
â”‚   â””â”€â”€ outputs/                             # Checkpoint
â”‚
â””â”€â”€ datasets/
    â”œâ”€â”€ kvasir_raw_6500_image_level_70_15_15/  # Exp1 data (random)
    â”‚   â”œâ”€â”€ train.json
    â”‚   â”œâ”€â”€ val.json
    â”‚   â””â”€â”€ test.json
    â””â”€â”€ kvasir_qwen_reordered_ultra_condensed/ # Exp2 data (ordered)
        â”œâ”€â”€ train.json
        â”œâ”€â”€ val.json
        â””â”€â”€ test.json
```

---

## âœ¨ Recommendation

### For Quick Baseline
**Run Exp1** to:
- Get baseline results quickly
- Establish performance floor
- Test infrastructure

### For Best Results
**Then run Exp2** to:
- Test intelligent ordering hypothesis
- Use larger/newer model (Qwen3-VL-8B)
- Achieve potentially higher accuracy

### Timeline
```
Hour 0: Submit Exp1
Hour 14: Exp1 completes â†’ Submit Exp2
Hour 28: Exp2 completes â†’ Compare results
Total: ~28 hours (~1.2 days) for both
```

---

## ğŸ“ Scientific Value

This is a **controlled experiment** testing ordering effect:
- âœ… Same images
- âœ… Same questions
- âœ… Same training time
- âœ… Same hardware
- âœ… Same resolution/preprocessing
- âš ï¸ Different: Order + Model + Instructions

**Result will show**: Does intelligent ordering + better model help?

---

*Comparison Date: November 11, 2025*  
*Both experiments use 768Ã—768 letterbox, 2 GPUs, image-level splits*  
*Key difference: Random order (exp1) vs Qwen clinical stages (exp2)*






