# Experiment 2: Qwen Reordered Training
# Strategy: Train on QA pairs reordered by Qwen into clinical stages (1,2,3)
# Model: Qwen3-VL-8B-Instruct
# Instructions: ULTRA_CONDENSED
# Resolution: 768x768 letterbox (aspect-ratio preserving)
# Hardware: 2 GPUs distributed training
# Expected time: ~14 hours

model_name: Qwen/Qwen3-VL-8B-Instruct

vision_frozen: true

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]

train:
  seed: 42
  max_seq_len: 1800       # For 768x768 resolution
  train_bs: 4             # Per GPU: 4 samples × 2 GPUs = 8 total per step
  grad_accum: 8           # Effective batch = 4×2×8 = 64
  eval_bs: 4
  lr: 1.0e-4
  weight_decay: 0.0
  epochs: 1
  warmup_ratio: 0.05
  bf16: true
  gradient_checkpointing: true
  logging_steps: 10
  save_steps: 500

data:
  # Qwen reordered data with ULTRA_CONDENSED instructions
  train_json: datasets/kvasir_qwen_reordered_ultra_condensed/train.json
  val_json: datasets/kvasir_qwen_reordered_ultra_condensed/val.json
  image_root: /l/users/muhra.almahri/Surgical_COT/datasets/Kvasir-VQA/raw/images
  
  # Letterbox settings (aspect-ratio preserving)
  use_letterbox: true
  target_size: 768         # 768x768 resolution

eval:
  numeric_rel_tol: 0.05
  numeric_abs_tol: 0.0

# ============================================================================
# EXP2 STRATEGY
# ============================================================================
# Train on Qwen-reordered QA pairs where a language model (Qwen) has
# intelligently reordered questions into 3 clinical stages:
#   Stage 1 (35%): Initial Assessment (quality, procedure, artifacts)
#   Stage 2 (64%): Findings Identification (abnormalities, instruments)
#   Stage 3 (0.1%): Clinical Context (diagnosis, treatment) - very few!
#
# The data is presented in Qwen's suggested clinical order but trained
# all together (not curriculum learning with separate checkpoints).
#
# This tests whether Qwen's clinical ordering helps vs random order (exp1).
# ============================================================================

# ============================================================================
# PERFORMANCE EXPECTATIONS (2 GPU distributed training)
# ============================================================================
# Resolution: 768×768 (letterbox, no warping)
# GPUs: 2 × A100 (or similar)
# Training time: ~14 hours
# Per-GPU batch: 4 samples
# Total batch per step: 8 samples (4 × 2 GPUs)
# Effective batch: 64 (with grad_accum=8)
# Memory per GPU: ~40-50 GB
# Expected accuracy: Better than exp1 random if Qwen ordering helps
# ============================================================================






