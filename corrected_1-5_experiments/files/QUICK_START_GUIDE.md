# üöÄ QUICK START GUIDE - Fix Your Surgical VQA Model

## üìã Summary

**Problem**: Model generates verbose explanations instead of short answers (loss = 7.26)

**Solution**: Add instruction fine-tuning to tell model HOW to answer

**Time to fix**: ~2 hours (30 min preprocessing + 1.5 hour training)

---

## ‚ö° Step-by-Step Instructions

### Step 1: Preprocess Your Data (30 minutes)

**Upload the preprocessing script to your cluster**:
```bash
# On your local machine (where you have the files I created)
scp preprocess_data_with_instructions.py muhra.almahri@your-cluster:/l/users/muhra.almahri/Surgical_COT/scripts/
```

**Run preprocessing**:
```bash
cd /l/users/muhra.almahri/Surgical_COT

python scripts/preprocess_data_with_instructions.py \
    --input_dir datasets/kvasir_raw_6500_image_level_70_15_15 \
    --output_dir datasets/kvasir_instructed \
    --files train.json val.json test.json
```

**Expected output**:
```
‚úì Saved 4550 processed items to: datasets/kvasir_instructed/train_instructed.json

Question Type Distribution:
  binary         : 1234 (27.1%)
  numeric        :  456 (10.0%)
  color          :  789 (17.3%)
  size           :  567 (12.5%)
  open_short     : 1234 (27.1%)
  open_long      :  270 ( 5.9%)
```

---

### Step 2: Modify Your Training Script (10 minutes)

**File**: `train_qwen_lora.py`

**Change 1** - Update `LazyVQADataset.__init__` (lines ~417-442):

Find this code:
```python
self.items.append({
    'question': item.get('question'),
    'answer': item.get('answer'),
    'image_id': item.get('image_id'),
    'image_path': image_path
})
```

Replace with:
```python
self.items.append({
    'question': item.get('question'),
    'answer': item.get('answer'),
    'instruction': item.get('instruction', item.get('question')),  # ‚Üê ADD
    'question_type': item.get('question_type', 'open_short'),     # ‚Üê ADD
    'image_id': item.get('image_id'),
    'image_path': image_path
})
```

**Change 2** - Update `LazyVQACollator.__call__` (lines ~459-520):

Find this code:
```python
def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
    images: List[Image.Image] = []
    questions: List[str] = []
    answers: List[str] = []

    # Extract data from features
    for feat in features:
        question = feat.get('question', '')  # ‚Üê OLD
        answer = feat.get('answer', '')
        image_path = feat.get('image_path')
        
        # ... image loading code ...
        
        questions.append(question)  # ‚Üê OLD
        answers.append(answer)

    # Build conversations
    messages_batch = []
    for question, answer in zip(questions, answers):  # ‚Üê OLD
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": question}  # ‚Üê OLD
                ]
            },
```

Replace with:
```python
def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
    images: List[Image.Image] = []
    instructions: List[str] = []  # ‚Üê CHANGED from questions
    answers: List[str] = []

    # Extract data from features
    for feat in features:
        instruction = feat.get('instruction', feat.get('question', ''))  # ‚Üê NEW
        answer = feat.get('answer', '')
        image_path = feat.get('image_path')
        
        # ... image loading code (no changes needed here) ...
        
        instructions.append(instruction)  # ‚Üê CHANGED
        answers.append(answer)

    # Build conversations
    messages_batch = []
    for instruction, answer in zip(instructions, answers):  # ‚Üê CHANGED
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": instruction}  # ‚Üê CHANGED
                ]
            },
```

**That's it!** Only 2 small changes needed.

---

### Step 3: Update Your SLURM Script (5 minutes)

**File**: `experiments/exp1_random/train_exp1.slurm`

Find the training command and update the data paths:

**OLD**:
```bash
python3 train_qwen_lora.py \
    --train_file datasets/kvasir_raw_6500_image_level_70_15_15/train.json \
    --val_file datasets/kvasir_raw_6500_image_level_70_15_15/val.json \
```

**NEW**:
```bash
python3 train_qwen_lora.py \
    --train_file datasets/kvasir_instructed/train_instructed.json \
    --val_file datasets/kvasir_instructed/val_instructed.json \
```

---

### Step 4: Train! (1.5 hours)

```bash
sbatch experiments/exp1_random/train_exp1.slurm
```

**Monitor training loss**:
```bash
tail -f experiments/exp1_random/logs/train_*.out
```

**What to expect**:
```
Step 100:  loss=5.234  ‚Üê Still learning
Step 200:  loss=3.156  ‚Üê Getting better
Step 300:  loss=2.012  ‚Üê Good progress!
Step 500:  loss=1.456  ‚Üê Excellent!
Step 1000: loss=0.892  ‚Üê Great!
```

**If loss stays > 4.0 after 500 steps** ‚Üí Something is wrong, check:
1. Are you using the `*_instructed.json` files?
2. Did you modify the collator to use `instruction` field?
3. Check the logs for errors

---

### Step 5: Evaluate with New Script

**Upload evaluation script**:
```bash
scp evaluate_improved.py muhra.almahri@your-cluster:/l/users/muhra.almahri/Surgical_COT/evaluation/
```

**Run evaluation**:
```bash
python evaluation/evaluate_improved.py \
    --model_path experiments/exp1_random/models/checkpoint-best \
    --test_data datasets/kvasir_instructed/test_instructed.json \
    --image_dir datasets/Kvasir-VQA/raw/images \
    --output results/exp1_improved_results.json \
    --base_model Qwen/Qwen2-VL-7B-Instruct
```

**Expected output**:
```
============================================================
EVALUATION RESULTS
============================================================

Overall Accuracy: 67.43%
Correct: 6055/8984

------------------------------------------------------------
Per Question Type Accuracy:
------------------------------------------------------------
binary         :  89.23% (2923/3275)
color          :  72.14% (1876/2601)
numeric        :  68.92% (445/646)
open_short     :  58.34% (811/1390)
size           :  55.21% (377/683)
...
```

---

## üéØ Before vs After Comparison

### Before (Current):
```
Question: "What is the color of the abnormality?"
Ground Truth: "pink"
Prediction: "The abnormality appears to be pink in color, indicating 
            possible inflammation or vascular changes in the tissue."
Loss: 7.26
Accuracy: 19.6%
```

### After (Fixed):
```
Question: "What is the color of the abnormality?"
Ground Truth: "pink"
Instruction: "You are a surgical image analysis assistant. Answer the 
             following question about the surgical/endoscopic image 
             with ONLY the color name. Provide a single word color. 
             Do not provide explanations.\n\nQuestion: What is the 
             color of the abnormality?\nAnswer with only the color:"
Prediction: "pink"
Loss: 1.2
Accuracy: 72%
```

---

## üìä Success Metrics

Your fix is working if you see:

‚úÖ Training loss drops below 2.0 within first epoch
‚úÖ Predictions are 1-3 words instead of paragraphs
‚úÖ Binary question accuracy > 85%
‚úÖ Overall accuracy > 60%
‚úÖ Model stops after giving the answer (no extra explanation)

---

## üîß Troubleshooting

### Problem: Loss still high (> 3.0 after 500 steps)

**Check**:
```bash
# Verify you're using instructed data
head -50 datasets/kvasir_instructed/train_instructed.json

# Should see "instruction" field with long template
# Should see "question_type" field
```

**Fix**: Make sure SLURM script points to `*_instructed.json` files

---

### Problem: Predictions still verbose

**Check training script modification**:
```python
# In LazyVQACollator.__call__, verify you have:
instruction = feat.get('instruction', feat.get('question', ''))  # ‚Üê Must be 'instruction'
```

**Not**:
```python
question = feat.get('question', '')  # ‚Üê Old way, wrong!
```

---

### Problem: "KeyError: instruction"

**Cause**: Dataset items don't have instruction field

**Fix**: 
1. Make sure preprocessing ran successfully
2. Check that `train_instructed.json` exists and has `instruction` field
3. Verify your training script uses `feat.get('instruction', ...)` with fallback

---

## üéì Understanding the Fix

**Why does this work?**

1. **Before**: Model saw `"What color?"` ‚Üí Doesn't know how to format answer
2. **After**: Model sees `"Answer with only the color:"` ‚Üí Clear instructions!

**The instruction template explicitly tells the model**:
- What type of answer is expected
- How to format it (single word, yes/no, etc.)
- NOT to provide explanations

This is called **instruction fine-tuning** - it's how ChatGPT, GPT-4, Claude, etc. all learned to follow instructions!

---

## üìù Next Steps

After Experiment 1 works:

1. **Apply same fix to Experiments 2-5**
   - Use same preprocessed data
   - Same code modifications
   - Just update paths in each SLURM script

2. **Compare results**
   - Curriculum learning (Exp 4) should now show clear benefits
   - Sequential ordering effects (Exp 2,3) will be more measurable
   - All should have much better accuracy than before

3. **Write up results**
   - You can now demonstrate that instruction tuning is critical
   - Clinical stage ordering effects can be properly measured
   - Your thesis has a clear contribution!

---

## üÜò Need Help?

If something doesn't work:

1. Check the full `DIAGNOSIS_AND_FIX_GUIDE.md` for detailed explanation
2. Verify each change was applied correctly
3. Look at training logs for error messages
4. Compare your code with the examples in this guide

Good luck! This fix should dramatically improve your results. üéâ
