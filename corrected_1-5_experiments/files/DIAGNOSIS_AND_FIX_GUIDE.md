# üîç COMPLETE DIAGNOSIS AND FIX GUIDE

## Your Current Situation

**Problem**: Model generates long explanatory responses instead of short answers
- Ground truth: `"pink"`
- Current prediction: `"The abnormality appears to be pink in color"`

**Training Loss**: 7.26 (very high - should be < 2.0)

**Accuracy**: Only 19.56% overall

---

## üêõ ROOT CAUSES IDENTIFIED

### 1. **NO INSTRUCTION FINE-TUNING** ‚ùå (CRITICAL)

**Location**: `train_qwen_lora.py`, lines 489-507 in `LazyVQACollator`

**Current Code**:
```python
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": question}  # ‚Üê Just raw question!
        ]
    },
    {
        "role": "assistant",
        "content": [{"type": "text", "text": answer}]
    }
]
```

**Problem**: The model gets NO guidance on HOW to answer. It sees:
- User: "What is the color of the abnormality?"
- Assistant: "pink"

The model learns to answer like ChatGPT with explanations because:
1. It was pre-trained to be conversational
2. Your training data doesn't override this behavior
3. There's no explicit instruction to be concise

**Why This Causes High Loss**:
- Model expects to generate ~50 tokens (like its pre-training)
- Training data only has 1 token ("pink")
- This mismatch creates high perplexity ‚Üí high loss

---

### 2. **NO QUESTION TYPE CLASSIFICATION** ‚ùå

**Problem**: Your data lacks `question_type` field

**Current data**:
```json
{
  "question": "What is the color of the abnormality?",
  "answer": "pink",
  "image_filename": "image.jpg"
}
```

**Missing**: How should the model know if it should output:
- A yes/no answer?
- A number?
- A single word?
- A description?

---

### 3. **EVALUATION USES FUZZY MATCHING** ‚ö†Ô∏è

**Location**: From your previous transcript evaluation code

**Current approach**:
```python
def smart_match(prediction: str, ground_truth: str, threshold: float = 0.7) -> bool:
    # Substring match
    if gt_n in pred_n or pred_n in gt_n:
        return True
```

**Problem**: This HIDES the real issue!
- Ground truth: `"no"`
- Prediction: `"The image appears to show... polyps have been removed"`
- Contains "no" ‚Üí Marked as CORRECT! ‚ùå

This is why some accuracy seems okay but model is actually broken.

---

## ‚úÖ THE COMPLETE FIX

### Step 1: Preprocess Your Data (Add Instructions)

**Run this command**:
```bash
python preprocess_data_with_instructions.py \
    --input_dir /l/users/muhra.almahri/Surgical_COT/datasets/kvasir_raw_6500_image_level_70_15_15 \
    --output_dir /l/users/muhra.almahri/Surgical_COT/datasets/kvasir_instructed \
    --files train.json val.json test.json
```

**What this does**:
1. Analyzes each question to determine its type
2. Adds appropriate instruction template
3. Creates new files: `train_instructed.json`, `val_instructed.json`, `test_instructed.json`

**Example Output**:
```json
{
  "question": "What is the color of the abnormality?",
  "answer": "pink",
  "question_type": "color",
  "instruction": "You are a surgical image analysis assistant. Answer the following question about the surgical/endoscopic image with ONLY the color name. Provide a single word color (e.g., 'red', 'pink', 'white', 'brown'). Do not provide explanations or additional text.\n\nQuestion: What is the color of the abnormality?\nAnswer with only the color:",
  "full_text": "You are a surgical image analysis assistant. ...\npink"
}
```

---

### Step 2: Modify Your Training Script

**Key Changes Needed in `LazyVQACollator.__call__`**:

**OLD (lines 489-507)**:
```python
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": question}  # ‚Üê Raw question
        ]
    },
    {
        "role": "assistant",
        "content": [{"type": "text", "text": answer}]
    }
]
```

**NEW**:
```python
# Use instruction field from preprocessed data
instruction = feat.get('instruction', question)  # Fallback to question if no instruction

messages = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": instruction}  # ‚Üê Use instruction template!
        ]
    },
    {
        "role": "assistant",
        "content": [{"type": "text", "text": answer}]
    }
]
```

**Complete modified collator section** (replace lines 459-520):
```python
def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
    images: List[Image.Image] = []
    instructions: List[str] = []
    answers: List[str] = []

    # Extract data from features
    for feat in features:
        # Use instruction if available, fallback to question
        instruction = feat.get('instruction', feat.get('question', ''))
        answer = feat.get('answer', '')
        image_path = feat.get('image_path')

        # Load image
        if image_path and os.path.exists(image_path):
            try:
                img = Image.open(image_path).convert('RGB')
                max_size = self.image_max_size
                if max(img.size) > max_size:
                    img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
                images.append(img)
            except Exception:
                images.append(Image.new('RGB', (self.image_max_size, self.image_max_size), color=(0, 0, 0)))
        else:
            images.append(Image.new('RGB', (self.image_max_size, self.image_max_size), color=(0, 0, 0)))

        instructions.append(instruction)
        answers.append(answer)

    # Build conversations with instruction templates
    messages_batch = []
    for instruction, answer in zip(instructions, answers):
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": instruction}  # ‚Üê Now uses proper instruction!
                ]
            },
            {
                "role": "assistant",
                "content": [{"type": "text", "text": answer}]
            }
        ]
        messages_batch.append(messages)

    # Apply chat template
    texts = [self.processor.apply_chat_template(msg, tokenize=False) 
             for msg in messages_batch]
    
    # Tokenize
    inputs = self.processor(
        text=texts,
        images=images,
        return_tensors="pt",
        padding=True
    )

    # Labels = input_ids (model handles masking)
    inputs['labels'] = inputs['input_ids'].clone()

    return inputs
```

---

### Step 3: Update LazyVQADataset to Load Instruction

**Modify lines 417-442**:

**OLD**:
```python
self.items.append({
    'question': item.get('question'),
    'answer': item.get('answer'),
    'image_id': item.get('image_id'),
    'image_path': image_path
})
```

**NEW**:
```python
self.items.append({
    'question': item.get('question'),
    'answer': item.get('answer'),
    'instruction': item.get('instruction', item.get('question')),  # ‚Üê Add this!
    'question_type': item.get('question_type', 'open_short'),     # ‚Üê Add this!
    'image_id': item.get('image_id'),
    'image_path': image_path
})
```

---

### Step 4: Update Training Command

**OLD command**:
```bash
python train_qwen_lora.py \
    --train_file datasets/kvasir_raw_6500_image_level_70_15_15/train.json \
    --val_file datasets/kvasir_raw_6500_image_level_70_15_15/val.json \
    ...
```

**NEW command** (use instructed data):
```bash
python train_qwen_lora.py \
    --train_file datasets/kvasir_instructed/train_instructed.json \
    --val_file datasets/kvasir_instructed/val_instructed.json \
    ...
```

---

### Step 5: Update Evaluation to Use Question Types

**Current evaluation** uses fuzzy matching which masks problems.

**You need**:
1. Strict exact matching for binary/numeric questions
2. Extract first word/number from predictions
3. Separate metrics per question type

I'll create a fixed evaluation script separately.

---

## üìä EXPECTED IMPROVEMENTS

### After Implementing Fixes:

| Metric | Before | After (Expected) |
|--------|--------|------------------|
| Training Loss | 7.26 | < 1.5 |
| Overall Accuracy | 19.6% | > 60% |
| Binary Q Accuracy | ~50% (random) | > 85% |
| Numeric Q Accuracy | Low | > 70% |
| Prediction Length | ~50 words | 1-3 words |

### Why This Will Work:

1. **Explicit Instructions** ‚Üí Model learns exact output format
2. **Question Type Awareness** ‚Üí Different handling per question
3. **Proper Training Signal** ‚Üí Loss decreases because predictions match labels
4. **Better Evaluation** ‚Üí See real performance without fuzzy matching hiding issues

---

## üöÄ IMPLEMENTATION CHECKLIST

- [ ] Run `preprocess_data_with_instructions.py` on your dataset
- [ ] Verify output has `instruction` and `question_type` fields
- [ ] Modify `LazyVQACollator.__call__` in training script (lines 459-520)
- [ ] Modify `LazyVQADataset.__init__` to load instruction field (lines 417-442)
- [ ] Update SLURM script paths to use `*_instructed.json` files
- [ ] Run Experiment 1 training with new data
- [ ] Monitor training loss (should drop to < 2.0 within first epoch)
- [ ] Run evaluation with new script (coming next)
- [ ] Compare predictions - should see short answers now!

---

## üí° WHY YOUR LOSS WAS SO HIGH

**Training Loss = 7.26 explained:**

```
Cross Entropy Loss = -log(P(correct_token))

Your scenario:
- Model sees question ‚Üí expects to generate ~50 tokens (its pre-training habit)
- Actual label is just 1 token: "pink"
- Model assigns very low probability to stopping after 1 token
- P(correct) ‚âà 0.0007 (very small)
- Loss = -log(0.0007) ‚âà 7.26

With instruction tuning:
- Model sees: "Answer with only the color:"
- Learns to stop after 1 token
- P(correct) ‚âà 0.2 (much higher)
- Loss = -log(0.2) ‚âà 1.6
```

---

## üéØ NEXT STEPS

1. **Immediate**: Run data preprocessing
2. **Today**: Modify training script and start training
3. **Tomorrow**: Monitor training metrics and evaluate
4. **This week**: Apply same fix to Experiments 2-5

Good luck! The fixes are straightforward and will dramatically improve your results.
